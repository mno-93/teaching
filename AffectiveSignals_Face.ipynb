{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AffectiveSignalsFace_Colab_II.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87sujXqUDRwh"
      },
      "source": [
        "# **Affective Signals - I**\n",
        "\n",
        "*Through this notebook we hope you get an hands on approach to the seminar. Below you will find code snippets and theory that will help you apply the libraries that will be introduced to you in the course.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi-sH5QlhXxd"
      },
      "source": [
        " # **First of all, please copy this Notebook in your Drive.** \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " Otherwise you are changing our code and we can see your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6u8ChlT3Pet"
      },
      "source": [
        "####**Install OpenFace**\n",
        "The first code snippet installs all the necessary dependencies and may take a while (30~40 minutes). Hence, everytime you close this notebook, you need to install all dependencies again to work on the notebook. Furthermore you need a Google Account, obviously.\n",
        "\n",
        "---\n",
        "\n",
        "During the installation, you will be asked about a configuration file, just type \"N\" for default action, and the installation will proceed. We added a sound hint shortly when you are asked for input, as well as you will hear a sound when the installation is done. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP0rO2jiCRKs"
      },
      "source": [
        "\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "from IPython.display import Audio\n",
        "sound_file = 'https://www.soundjay.com/buttons/sounds/button-2.mp3'\n",
        "\n",
        "################# Need to revert back to CUDA 10.0 ##################\n",
        "# Thanks to http://aconcaguasci.blogspot.com/2019/12/setting-up-cuda-100-for-mxnet-on-google.html\n",
        "#Uninstall the current CUDA version\n",
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update\n",
        "\n",
        "#Download CUDA 10.0\n",
        "!wget  --no-clobber https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "Audio(sound_file, autoplay=True)\n",
        "#install CUDA kit dpkg\n",
        "!dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "!sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-10-0\n",
        "#Slove libcurand.so.10 error\n",
        "!wget --no-clobber http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "#-nc, --no-clobber: skip downloads that would download to existing files.\n",
        "!apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "!apt-get update\n",
        "####################################################################\n",
        "\n",
        "git_repo_url = 'https://github.com/TadasBaltrusaitis/OpenFace.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]n\n",
        "\n",
        "\n",
        "# clone openface\n",
        "!git clone -q --depth 1 $git_repo_url\n",
        "\n",
        "# install new CMake becaue of CUDA10\n",
        "!wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "!tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "\n",
        "# Get newest GCC\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential \n",
        "!sudo apt-get install g++-8\n",
        "\n",
        "# install python dependencies\n",
        "#!pip install -q youtube-dl\n",
        "\n",
        "# Finally, actually install OpenFace\n",
        "!cd OpenFace && bash ./download_models.sh && sudo bash ./install.sh\n",
        "Audio(sound_file, autoplay=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HznkvuMe50TB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aef5501-e8b0-4c24-8893-7088a21a692f"
      },
      "source": [
        "# mount your Google Drive, where your video is located\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEsF9UVR6VI"
      },
      "source": [
        "# Neuer Abschnitt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5MVNPKmDaWE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import re\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhgdbNlsDdrv"
      },
      "source": [
        "# Facial Expression\n",
        "\n",
        "We tend to use facial expressions to communicate with each other and also express our emotional state. A facial expression is one or more motions or positions of the muscles beneath the skin of the face. These movements are believed convey the emotional state of an individual to observers. Thus, facial expressions are one form of nonverbal communication.[Source](https://en.wikipedia.org/wiki/Facial_expression)\n",
        "\n",
        "\n",
        "## [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace/wiki)\n",
        "\n",
        "This tool will be extensively used in this notebook to extract relevant information from your videos. Below you will find code snippets to process your videos using the OpenFace - Library that you were introduced to in the seminar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV7VseHYD6pX"
      },
      "source": [
        "#path to your videos should be added here\n",
        "\n",
        "#videos = \"path to your video\"\n",
        "\n",
        "# e.g.\n",
        "videos = \"/content/drive/MyDrive/AffectiveSignals/video.mp4\"\n",
        "\n",
        "# create a subfolder for your video directory called e.g. \"ExtractedFeatures\"\n",
        "output_dir = \"/content/drive/MyDrive/AffectiveSignals/ExtractedFeatures/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KPgrBW7FAbG"
      },
      "source": [
        "# Run OpenFace on your Video\n",
        "\n",
        "!./OpenFace/build/bin/FeatureExtraction -f \"$videos\" -out_dir \"$output_dir\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr0nQ2C-lz0p"
      },
      "source": [
        "---\n",
        "\n",
        "You can find the processed video in the directory that you ran the jupyter-notebook, you can import the processed video and play it as shown below.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZPMB3GRtPPn"
      },
      "source": [
        "# Display video with extracted features\n",
        "\n",
        "def show_local_mp4_video(file_name, width=640, height=480):\n",
        "  import io\n",
        "  import base64\n",
        "  from IPython.display import HTML\n",
        "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(width, height, video_encoded.decode('ascii')))\n",
        "\n",
        "#covert the processed file to mp4 so it can be displayed in a browser (it takes a couple of seconds)\n",
        "\n",
        "# e.g.\n",
        "!ffmpeg -y -loglevel info -i \"/content/drive/MyDrive/AffectiveSignals/ExtractedFeatures/video.avi\" \"/content/drive/MyDrive/AffectiveSignals/ExtractedFeatures/video.mp4\"\n",
        "\n",
        "#!ffmpeg -y -loglevel info -i \"your_avi_file_directory\" \"your_output_dir\"\n",
        "\n",
        "# display video\n",
        "\n",
        "# e.g\n",
        "show_local_mp4_video(\"/content/drive/MyDrive/AffectiveSignals/ExtractedFeatures/1620144600921.mp4\", width=960, height=720)\n",
        "\n",
        "#show_local_mp4_video(\"path to your processed video\", width=960, height=720)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uJ72QHKvrrv"
      },
      "source": [
        "----\n",
        "OpenFace also creates a csv file that contains all features that have been extracted from the video.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f66100z0vv4F"
      },
      "source": [
        "#load the csv file using pandas\n",
        "# e.g\n",
        "df_clean = pd.read_csv(\"/content/drive/MyDrive/AffectiveSignals/ExtractedFeatures/video.csv\")\n",
        "\n",
        "#df_clean = pd.read_csv(\"path to your generated csv file\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK509OaHv7Rz"
      },
      "source": [
        "#Short view of all the contents of the file \n",
        "df_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0Yd3GgzEVcs"
      },
      "source": [
        "#### **Action Units**\n",
        "Facial Action Coding System (FACS) is a system to taxonomize human facial movements by their appearance on the face. Movements of individual facial muscles are encoded by FACS from slight different instant changes in facial appearance. Using FACS it is possible to code nearly any anatomically possible facial expression, deconstructing it into the specific Action Units (AU) that produced the expression. It is a common standard to objectively describe facial expressions [[1]](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units)\n",
        "\n",
        "List of all the Action units can be found [here](https://www.cs.cmu.edu/~face/facs.htm). You can find small animation of the action units [here](https://imotions.com/blog/facial-action-coding-system/).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Let's look at the intensity plots of all the action units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3j5mdMzwlj1"
      },
      "source": [
        "# Threshold data by 80%\n",
        "df_clean = df_clean[df_clean.confidence>=.80]\n",
        "# Plot all Action Unit time series. \n",
        "au_regex_pat = re.compile(r'^AU[0-9]+_r$')\n",
        "au_columns_r = df_clean.columns[df_clean.columns.str.contains(au_regex_pat)]\n",
        "print(\"List of AU columns:\", au_columns_r)\n",
        "f,axes = plt.subplots(6, 3, figsize=(10,12), sharex=True, sharey=True)\n",
        "axes = axes.flatten()\n",
        "for au_ix, au_col in enumerate(au_columns_r):\n",
        "    sns.lineplot(x='frame', y=au_col, hue='face_id', data=df_clean, ax=axes[au_ix])\n",
        "    axes[au_ix].set(title=au_col, ylabel='Intensity')\n",
        "    axes[au_ix].legend(loc=5)\n",
        "plt.suptitle(\"AU intensity predictions\", y=1.02)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYDELf9w0m4J"
      },
      "source": [
        "au_regex_pat = re.compile(r'^AU[0-9]+_c$')\n",
        "au_columns = df_clean.columns[df_clean.columns.str.contains(au_regex_pat)]\n",
        "print(\"List of AU columns:\", au_columns)\n",
        "f,axes = plt.subplots(6, 3, figsize=(10,12), sharex=True, sharey=True)\n",
        "axes = axes.flatten()\n",
        "for au_ix, au_col in enumerate(au_columns):\n",
        "    sns.lineplot(x='frame', y=au_col, hue='face_id', data=df_clean, ax=axes[au_ix])\n",
        "    axes[au_ix].set(title=au_col, ylabel=\"Presence\")\n",
        "    axes[au_ix].legend(loc=5)\n",
        "plt.suptitle(\"AU occurence predictions\", y=1.02)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUVTrh3iMI78"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We know that AU45 corresponds to eye blinks. Let's try to find out how many times do you blink in your own video based on the presence of the according AU. Use the .csv provided by the OpenFace-Library!\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42OTh2uv00Op"
      },
      "source": [
        "**Task 1.1: Count the number of times the action unit AU45 occurs in your video**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJMUa_JQ0-Ad"
      },
      "source": [
        "AU12 and AU6 correspond to happiness, so let's see how many times do they co-occur in your video:\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6XaMRNbhM2S"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lb0pQ321PJ1"
      },
      "source": [
        "**Task 1.2: Count the number of times the action units 12 and 6 are present simultaneously in your video**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAAaZxXy1RY7"
      },
      "source": [
        "#your code goes here:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIZ4t1qo1Ue2"
      },
      "source": [
        "---\n",
        "\n",
        "**Task 1.3: Visualize the correlation of all the Action Units that are present based on their intensity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn2flTBr1YeQ"
      },
      "source": [
        "#your code goes here (hint: try to use AU_columns_r variables to get the required columns and find the correlation matrix from the dataframe):\n",
        "\n",
        "\n",
        "\n",
        "#once you get you corelation you can use the below function to plot the matrix\n",
        "sns.heatmap(corr, \n",
        "            xticklabels=corr.columns.values,\n",
        "            yticklabels=corr.columns.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbZ9SwRL1cZP"
      },
      "source": [
        "---\n",
        "\n",
        "**Task 1.4: Calculate the mean and variance for each of the Action Unit intensities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1068z2a1lYy"
      },
      "source": [
        "#your code goes here:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK4KJ6xO1nAL"
      },
      "source": [
        "If you are interested, try out additional analyses and visualizations to explore your individual facial expressions in the video. For example, try to find out how much you're smiling in your video. We can discuss your findings and ideas in the upcoming seminar!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBhh4JAg1qi2"
      },
      "source": [
        "#a place for additional interesting analyses:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CxFOswW1tQD"
      },
      "source": [
        "**-----------------END OF FACIAL EXPRESSIONS-------------------**"
      ]
    }
  ]
}